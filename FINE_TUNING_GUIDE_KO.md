# Qwen3 파인튜닝 가능성 및 권장 접근

질문 요약: **에피타이저 1 + 메인 1 + 드링크 1 + 디저트 1** 조합(총 1만 경우)에 대해, 현재 준비한 **25개 예시 답안**으로 Qwen3를 파인튜닝해서 원하는 형식/품질의 답변을 안정적으로 만들 수 있는가?

## 결론 (짧게)
- **형식 학습(출력 템플릿 고정)은 가능**합니다.
- 하지만 **평가 품질(점수 일관성, 페어링 논리 일반화)**까지 기대하려면 25개는 매우 부족합니다.
- 실전에서는 `프롬프트 + 규칙/점수 로직` 또는 `RAG + 소량 SFT` 혼합이 더 안정적입니다.

---

## 왜 25개로는 부족한가?
1. 조합 공간이 큽니다. (1만 경우)
2. 현재 데이터는 예시 스타일은 잘 보여주지만, 모델이 일반화할 근거(다양한 상호작용 패턴)가 적습니다.
3. 점수(각 20점, 총 100점)까지 정확히 맞추려면 **일관된 채점 기준**이 충분히 관찰되어야 합니다.

즉, 25개로는 모델이 “문체”는 모방해도, 낯선 조합에서 “설득력 있는 점수 근거”를 안정적으로 만들기 어렵습니다.

---

## 권장 전략

### 1) 1단계: 파인튜닝 없이 베이스라인 구축
- 시스템 프롬프트에 출력 형식과 채점 기준을 명시
- few-shot 예시(현재 25개 중 대표 5~8개) 제공
- 온도 낮춤(예: `temperature=0.2~0.4`)으로 일관성 확보

### 2) 2단계: 데이터 확장 후 LoRA/SFT
- 최소 수백 개(권장 300~1000+)로 확장
- 각 축(가벼움/무거움, 산미/지방, 단맛/짠맛 등)이 고르게 섞이도록 샘플링
- 포맷은 반드시 고정

### 3) 3단계: 평가 루프
- 정답셋(검증셋) 별도 보관
- 포맷 적합성, 점수 분포, 설명-점수 정합성 평가
- 실패 케이스를 재학습 데이터에 반영

---

## 지금 레포에서 바로 할 수 있는 것
아래 스크립트로 `Food combination.txt`를 Qwen SFT용 chat JSONL(`train.jsonl`, `val.jsonl`)로 변환할 수 있습니다.

```bash
python prepare_finetune_dataset.py \
  --input "Food combination.txt" \
  --train-output train.jsonl \
  --val-output val.jsonl
```

이 스크립트는:
- `example n)` 블록을 파싱
- 메뉴 입력을 user message로 구성
- 원문 답안을 assistant message로 보존
- train/val 분할까지 자동 처리

---

## 실무 팁
- 25개만으로 시작한다면 “파인튜닝”보다 먼저 “프롬프트 고정 + few-shot” 성능을 측정하세요.
- 파인튜닝을 하더라도 **규칙 기반 점수 보정기**(예: sweetness overlap 패널티)를 두면 품질이 크게 안정됩니다.
- 모델이 점수 합계를 자주 틀리면, 후처리에서 총점을 재계산해 교정하는 방법도 유효합니다.

---

## 한 줄 답변
**가능은 하지만, 25개만으로 1만 조합의 ‘모범답안 품질’을 안정적으로 일반화하기는 어렵고, 데이터 확장 + 프롬프트/규칙 혼합 접근이 현실적으로 더 좋습니다.**
