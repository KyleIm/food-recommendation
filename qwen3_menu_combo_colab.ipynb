{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Rr8CGz0bST1"
   },
   "source": [
    "# Qwen3(4B) 메뉴 조합 점수화 노트북 (Unsloth 스타일)\n",
    "\n",
    "이 노트북은 **Qwen3-4B-Instruct**를 사용해 아래 4개 카테고리에서 1개씩 뽑은 메뉴 조합의 점수를 계산합니다.\n",
    "\n",
    "- Appetizer\n",
    "- Main Dish\n",
    "- Drink\n",
    "- Dessert\n",
    "\n",
    "> Colab에서 바로 실행 가능한 형태로 구성했습니다.\n"
   ],
   "id": "2Rr8CGz0bST1"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3HIULAGBbST9"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os, re\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth  # Do this in local & cloud setups\n",
    "else:\n",
    "    import torch; v = re.match(r'[\\d]{1,}\\.[\\d]{1,}', str(torch.__version__)).group(0)\n",
    "    xformers = 'xformers==' + {'2.10':'0.0.34','2.9':'0.0.33.post1','2.8':'0.0.32.post2'}.get(v, \"0.0.34\")\n",
    "    !pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "    !pip install --no-deps unsloth_zoo bitsandbytes accelerate {xformers} peft trl triton unsloth\n",
    "!pip install transformers==4.56.2\n",
    "!pip install --no-deps trl==0.22.2"
   ],
   "id": "3HIULAGBbST9"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lczvLoqkbSUA"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import itertools\n",
    "from typing import Dict, List\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n"
   ],
   "id": "lczvLoqkbSUA"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "I25nL6YibSUB"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1) 사용자 지정 데이터셋\n",
    "DATASET = {\n",
    "    \"appetizer\": [\n",
    "        \"salad\", \"corn soup\", \"miso soup\", \"house bread\", \"cheese\",\n",
    "        \"cracker\", \"Scotch Egg\", \"mashed potato\", \"nachos\", \"pasta\"\n",
    "    ],\n",
    "    \"main_dish\": [\n",
    "        \"ramen\", \"pizza\", \"fried chicken\", \"sandwich\", \"T-bone steak\",\n",
    "        \"sushi\", \"taco\", \"grilled tofu\", \"fish and chips\", \"paella\"\n",
    "    ],\n",
    "    \"drink\": [\n",
    "        \"coca-cola\", \"red wine\", \"white wine\", \"sake\", \"green tea\",\n",
    "        \"orange juice\", \"coke zero\", \"modelo beer\", \"coffee\", \"water\"\n",
    "    ],\n",
    "    \"dessert\": [\n",
    "        \"orange\", \"grape\", \"pudding\", \"ice cream\", \"tart\",\n",
    "        \"cheesecake\", \"macaron\", \"dango\", \"muffin\", \"churro\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "print({k: len(v) for k, v in DATASET.items()})\n",
    "print(\"총 가능한 조합 수:\", len(DATASET[\"appetizer\"]) * len(DATASET[\"main_dish\"]) * len(DATASET[\"drink\"]) * len(DATASET[\"dessert\"]))\n"
   ],
   "id": "I25nL6YibSUB"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8EIsu7-QbSUC"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 2) Qwen3 로드 (Unsloth template)\n",
    "max_seq_length = 2048\n",
    "candidate_models = [\n",
    "    \"unsloth/Qwen3-4B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Qwen3-4B-unsloth-bnb-4bit\",\n",
    "    \"Qwen/Qwen3-4B-Instruct\",\n",
    "]\n",
    "\n",
    "last_error = None\n",
    "model = None\n",
    "tokenizer = None\n",
    "model_name = None\n",
    "\n",
    "for candidate in candidate_models:\n",
    "    try:\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name = candidate,\n",
    "            max_seq_length = max_seq_length,\n",
    "            dtype = None,      # 자동 선택\n",
    "            load_in_4bit = True,\n",
    "        )\n",
    "        model_name = candidate\n",
    "        break\n",
    "    except Exception as e:\n",
    "        last_error = e\n",
    "        print(f\"[skip] {candidate}: {e}\")\n",
    "\n",
    "if model is None:\n",
    "    raise RuntimeError(\n",
    "        \"Qwen3 모델 로드 실패. candidate_models의 이름을 확인하거나,\"\n",
    "        \"허깅페이스 접근 권한/네트워크 상태를 확인하세요.\"\n",
    "    ) from last_error\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "print(\"Loaded:\", model_name)\n"
   ],
   "id": "8EIsu7-QbSUC"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FDXlM30WbSUD"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are a professional menu-pairing evaluator.\n",
    "Follow the style learned from the provided reference examples.\n",
    "Given exactly one appetizer, one main dish, one drink, and one dessert,\n",
    "write the response in this exact structure:\n",
    "\n",
    "Appetizer: <item>\n",
    "Main Dish: <item>\n",
    "Drink: <item>\n",
    "Dessert: <item>\n",
    "\n",
    "Appetizer–Main Dish Balance (x/20): <1-2 sentence rationale>\n",
    "\n",
    "Main Dish–Drink Balance (x/20): <1-2 sentence rationale>\n",
    "\n",
    "Main Dish–Dessert Balance (x/20): <1-2 sentence rationale>\n",
    "\n",
    "Appetizer–Dessert Balance (x/20): <1-2 sentence rationale>\n",
    "\n",
    "Drink–Dessert Balance (x/20): <1-2 sentence rationale>\n",
    "\n",
    "Total Score: <0-100>/100\n",
    "\n",
    "Overall Evaluation: <concise summary>\n",
    "\n",
    "Rules:\n",
    "- Output only the final answer.\n",
    "- No <think> tags.\n",
    "- Keep section order and labels exactly the same.\n",
    "- Use English.\n",
    "\"\"\"\n",
    "\n",
    "REFERENCE_FILE_CANDIDATES = [\n",
    "    Path(\"Food combination.txt\"),\n",
    "    Path(\"/content/Food combination.txt\"),\n",
    "    Path(\"/content/food-recommendation/Food combination.txt\"),\n",
    "]\n",
    "\n",
    "REFERENCE_FILE_URLS = [\n",
    "    \"https://raw.githubusercontent.com/KyleIm/food-recommendation/main/Food%20combination.txt\",\n",
    "]\n",
    "\n",
    "\n",
    "def _load_reference_text() -> str:\n",
    "    import urllib.request\n",
    "\n",
    "    for candidate in REFERENCE_FILE_CANDIDATES:\n",
    "        if candidate.exists():\n",
    "            print(f\"Using reference template file: {candidate}\")\n",
    "            return candidate.read_text(encoding=\"utf-8\")\n",
    "\n",
    "    # Colab에서 로컬 파일이 없으면 GitHub raw에서 자동 가져오기\n",
    "    for url in REFERENCE_FILE_URLS:\n",
    "        try:\n",
    "            print(f\"Reference file not found locally. Trying download: {url}\")\n",
    "            with urllib.request.urlopen(url, timeout=15) as resp:\n",
    "                text = resp.read().decode(\"utf-8\")\n",
    "            if \"example 1)\" in text.lower() or \"Appetizer:\" in text:\n",
    "                out = Path(\"/content/Food combination.txt\")\n",
    "                out.write_text(text, encoding=\"utf-8\")\n",
    "                print(f\"Downloaded reference template file: {out}\")\n",
    "                return text\n",
    "        except Exception as e:\n",
    "            print(f\"[skip] download failed: {e}\")\n",
    "\n",
    "    raise FileNotFoundError(\n",
    "        \"Food combination.txt not found locally and download failed. \"\n",
    "        \"Upload the file to /content or current directory.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def parse_reference_examples(raw_text: str):\n",
    "    import re\n",
    "\n",
    "    blocks = [b.strip() for b in re.split(r\"\\bexample\\s+\\d+\\)\\s*\", raw_text, flags=re.IGNORECASE) if b.strip()]\n",
    "    parsed = []\n",
    "    field_re = re.compile(\n",
    "        r\"Appetizer:\\s*(?P<app>.+?)\\n\"\n",
    "        r\"Main Dish:\\s*(?P<main>.+?)\\n\"\n",
    "        r\"Drink:\\s*(?P<drink>.+?)\\n\"\n",
    "        r\"Dessert:\\s*(?P<dessert>.+?)\\n\",\n",
    "        re.DOTALL,\n",
    "    )\n",
    "\n",
    "    for i, block in enumerate(blocks, 1):\n",
    "        m = field_re.search(block)\n",
    "        if not m:\n",
    "            continue\n",
    "        user = (\n",
    "            \"Evaluate this menu combination with the required format:\\n\"\n",
    "            f\"Appetizer: {m.group('app').strip()}\\n\"\n",
    "            f\"Main Dish: {m.group('main').strip()}\\n\"\n",
    "            f\"Drink: {m.group('drink').strip()}\\n\"\n",
    "            f\"Dessert: {m.group('dessert').strip()}\"\n",
    "        )\n",
    "        parsed.append({\"user\": user, \"assistant\": block.strip()})\n",
    "\n",
    "    if not parsed:\n",
    "        raise ValueError(\"No valid examples parsed from Food combination.txt\")\n",
    "    return parsed\n",
    "\n",
    "\n",
    "REFERENCE_EXAMPLES = parse_reference_examples(_load_reference_text())\n",
    "print(f\"Loaded reference examples: {len(REFERENCE_EXAMPLES)}\")\n",
    "\n",
    "\n",
    "def build_user_prompt(combo: Dict[str, str]) -> str:\n",
    "    return (\n",
    "        \"Evaluate this menu combination with the required format:\\n\"\n",
    "        f\"Appetizer: {combo['appetizer']}\\n\"\n",
    "        f\"Main Dish: {combo['main_dish']}\\n\"\n",
    "        f\"Drink: {combo['drink']}\\n\"\n",
    "        f\"Dessert: {combo['dessert']}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def build_messages_with_references(combo: Dict[str, str], n_shots: int = 3):\n",
    "    messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n",
    "\n",
    "    # 앞쪽 예시 n개를 few-shot으로 넣어 템플릿 문체를 강하게 고정\n",
    "    for ex in REFERENCE_EXAMPLES[: max(1, n_shots)]:\n",
    "        messages.append({\"role\": \"user\", \"content\": ex[\"user\"]})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": ex[\"assistant\"]})\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": build_user_prompt(combo)})\n",
    "    return messages\n",
    "\n",
    "\n",
    "def strip_think(text: str) -> str:\n",
    "    import re\n",
    "\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    return re.sub(r\"<think>[\\s\\S]*?</think>\", \"\", text, flags=re.IGNORECASE).strip()\n",
    "\n",
    "\n",
    "def extract_formatted_answer(text: str) -> str:\n",
    "    import re\n",
    "\n",
    "    cleaned = strip_think(text)\n",
    "    # Appetizer 시작부터 Overall Evaluation까지 잘라냄\n",
    "    pattern = re.compile(\n",
    "        r\"(Appetizer:[\\s\\S]*?Overall Evaluation:\\s*.+)\",\n",
    "        flags=re.IGNORECASE,\n",
    "    )\n",
    "    m = pattern.search(cleaned)\n",
    "    return m.group(1).strip() if m else cleaned\n",
    "\n",
    "\n",
    "def extract_total_score(text: str):\n",
    "    import re\n",
    "\n",
    "    cleaned = extract_formatted_answer(text)\n",
    "    m = re.search(r\"Total Score:\\s*(\\d{1,3})\\s*/\\s*100\", cleaned, flags=re.IGNORECASE)\n",
    "    if not m:\n",
    "        return None\n",
    "    return max(0, min(100, int(m.group(1))))\n",
    "\n",
    "\n",
    "def repair_to_template(combo: Dict[str, str], raw_text: str, max_new_tokens: int = 512):\n",
    "    repair_messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"Rewrite the following draft into the exact required template. \"\n",
    "                \"Return only final answer with the fixed labels and section order.\"\n",
    "            ),\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": build_user_prompt(combo)},\n",
    "        {\"role\": \"assistant\", \"content\": REFERENCE_EXAMPLES[0][\"assistant\"]},\n",
    "        {\"role\": \"user\", \"content\": f\"Draft to rewrite:\\n{strip_think(raw_text)}\"},\n",
    "    ]\n",
    "\n",
    "    repair_inputs = tokenizer.apply_chat_template(\n",
    "        repair_messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(model.device)\n",
    "\n",
    "    attention_mask = repair_inputs.ne(tokenizer.pad_token_id) if tokenizer.pad_token_id is not None else None\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        repair_outputs = model.generate(\n",
    "            input_ids=repair_inputs,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            temperature=0.2,\n",
    "            repetition_penalty=1.05,\n",
    "        )\n",
    "\n",
    "    repaired = tokenizer.decode(repair_outputs[0][repair_inputs.shape[-1]:], skip_special_tokens=True)\n",
    "    return extract_formatted_answer(repaired)\n",
    "\n",
    "\n",
    "def evaluate_combo_with_qwen3(combo: Dict[str, str], max_new_tokens: int = 700, n_shots: int = 3):\n",
    "    messages = build_messages_with_references(combo, n_shots=n_shots)\n",
    "\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(model.device)\n",
    "\n",
    "    attention_mask = inputs.ne(tokenizer.pad_token_id) if tokenizer.pad_token_id is not None else None\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            temperature=0.2,\n",
    "            repetition_penalty=1.05,\n",
    "        )\n",
    "\n",
    "    generated = tokenizer.decode(outputs[0][inputs.shape[-1]:], skip_special_tokens=True)\n",
    "    response = extract_formatted_answer(generated)\n",
    "    total_score = extract_total_score(response)\n",
    "\n",
    "    # 템플릿이 깨졌으면 1회 보정\n",
    "    if total_score is None or \"Overall Evaluation:\" not in response:\n",
    "        response = repair_to_template(combo, generated)\n",
    "        total_score = extract_total_score(response)\n",
    "\n",
    "    return {\"total_score\": total_score, \"response\": response}\n",
    "\n",
    "\n"
   ],
   "id": "FDXlM30WbSUD"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GH8jglYObSUD"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 3) 카테고리별 1개씩 랜덤으로 뽑아 형식 출력 확인\n",
    "\n",
    "def sample_combo(seed: int = None) -> Dict[str, str]:\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "    return {\n",
    "        \"appetizer\": random.choice(DATASET[\"appetizer\"]),\n",
    "        \"main_dish\": random.choice(DATASET[\"main_dish\"]),\n",
    "        \"drink\": random.choice(DATASET[\"drink\"]),\n",
    "        \"dessert\": random.choice(DATASET[\"dessert\"]),\n",
    "    }\n",
    "\n",
    "combo = sample_combo(seed=42)\n",
    "result = evaluate_combo_with_qwen3(combo)\n",
    "\n",
    "print(\"선택된 조합:\")\n",
    "print(combo)\n",
    "print(\"\\n모델 출력:\")\n",
    "print(result[\"response\"])\n",
    "print(\"\\n추출된 Total Score:\", result[\"total_score\"])\n",
    "\n"
   ],
   "id": "GH8jglYObSUD"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "X7uA7DfUbSUE"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "# (옵션) 랜덤 3개 조합 평가 후 Total Score 기준 정렬\n",
    "\n",
    "def evaluate_three_random(seed: int = 0) -> List[Dict]:\n",
    "    random.seed(seed)\n",
    "    rows = []\n",
    "\n",
    "    for _ in range(3):\n",
    "        combo = sample_combo()\n",
    "        result = evaluate_combo_with_qwen3(combo)\n",
    "        rows.append({\n",
    "            \"combo\": combo,\n",
    "            \"total_score\": result[\"total_score\"],\n",
    "            \"response\": result[\"response\"],\n",
    "        })\n",
    "\n",
    "    rows.sort(key=lambda x: x[\"total_score\"] if x[\"total_score\"] is not None else -1, reverse=True)\n",
    "    return rows\n",
    "\n",
    "ranked = evaluate_three_random(seed=7)\n",
    "for i, row in enumerate(ranked, 1):\n",
    "    print(f\"#{i} | total_score={row['total_score']} | {row['combo']}\")\n",
    "    print(row['response'])\n",
    "    print()\n",
    "\n"
   ],
   "id": "X7uA7DfUbSUE"
  }
 ],
 "metadata": {
  "colab": {
   "name": "qwen3_menu_combo_colab.ipynb",
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}